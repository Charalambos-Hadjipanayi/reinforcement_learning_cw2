{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Easy21:\n",
    "\n",
    "    def __init__(self, max_length=1000):\n",
    "        self.max_length = max_length\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.player_first_card_val = np.random.choice(10) + 1\n",
    "        self.dealer_first_card_val = np.random.choice(10) + 1\n",
    "\n",
    "        self.player_sum = self.player_first_card_val\n",
    "        self.dealer_sum = self.dealer_first_card_val\n",
    "\n",
    "        self.state = [self.dealer_first_card_val, self.player_sum]\n",
    "\n",
    "        self.player_goes_bust = False\n",
    "        self.dealer_goes_bust = False\n",
    "\n",
    "        self.ret = 0\n",
    "        self.terminal = False\n",
    "        self.t = 0\n",
    "\n",
    "        return self.state\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        # action 1: hit   0: stick\n",
    "        # color: 1: black   -1: red\n",
    "        r = 0\n",
    "\n",
    "        if action == 1:\n",
    "            self.player_card_val = np.random.choice(10) + 1\n",
    "            self.player_card_col = np.random.choice([-1, 1], p=[1./3., 2./3.])\n",
    "\n",
    "            self.player_sum += (self.player_card_val * self.player_card_col)\n",
    "            self.player_goes_bust = self.check_go_bust(self.player_sum)\n",
    "\n",
    "            if self.player_goes_bust == 1:\n",
    "                r = -1\n",
    "                self.terminal = True\n",
    "\n",
    "        if not self.terminal and self.dealer_sum < 17:\n",
    "            self.dealer_card_val = np.random.choice(10) + 1\n",
    "            self.dealer_card_col = np.random.choice([-1, 1], p=[1./3., 2./3.])\n",
    "\n",
    "            self.dealer_sum += (self.dealer_card_val * self.dealer_card_col)\n",
    "            self.dealer_goes_bust = self.check_go_bust(self.dealer_sum)\n",
    "\n",
    "            if self.dealer_goes_bust == 1:\n",
    "                r = 1\n",
    "                self.terminal = True\n",
    "\n",
    "        self.t += 1\n",
    "        self.ret += r\n",
    "\n",
    "        if not self.terminal and self.t == self.max_length:\n",
    "            self.terminal = True\n",
    "            if self.player_sum > self.dealer_sum: r = 1\n",
    "            elif self.player_sum < self.dealer_sum: r = -1\n",
    "\n",
    "        if self.terminal: return 'Terminal', r, self.terminal\n",
    "        else:\n",
    "            self.state[1] = self.player_sum\n",
    "            return self.state, r, self.terminal\n",
    "\n",
    "\n",
    "    def check_go_bust(self, Sum):\n",
    "        return bool(Sum > 21 or Sum < 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte Carlo -- one episode\n",
    "def monte_carlo(Q, Returns, count_state, count_state_action):\n",
    "    appeared = np.zeros([10, 21, 2], dtype=int)\n",
    "\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    s = env.reset()\n",
    "    states = [s]\n",
    "\n",
    "    while True:\n",
    "        action_greedy = Q[s[0]-1, s[1]-1, :].argmax()\n",
    "        count_state[s[0]-1, s[1]-1] += 1\n",
    "        epsilon = count_constant / float(count_constant + count_state[s[0]-1, s[1]-1])\n",
    "        action = np.random.choice([action_greedy, 1 - action_greedy], p=[1. - epsilon/2., epsilon/2.])\n",
    "        actions.append(action)\n",
    "\n",
    "        s, r, term = env.step(action=action)\n",
    "        rewards.append(r)\n",
    "\n",
    "        if term: break\n",
    "        else: states.append(s)\n",
    "\n",
    "    for t in range(len(states)):\n",
    "        \n",
    "        ## ================== change here ================== ##\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        ## ================================================= ##\n",
    "    \n",
    "    return Q, Returns, count_state, count_state_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte Carlo\n",
    "Q_MC = np.zeros([10, 21, 2]) # Q(s, a)\n",
    "Returns = np.zeros([10, 21, 2]) # empirical first-visit returns\n",
    "count_state_action = np.zeros([10, 21, 2], dtype=int) # N(s, a)\n",
    "count_state = np.zeros([10, 21], dtype=int) # N(s)\n",
    "count_constant = 100\n",
    "\n",
    "n_episodes = 20000\n",
    "env = Easy21()\n",
    "\n",
    "for i_epi in range(n_episodes):\n",
    "    Q_MC, Returns, count_state, count_state_action = monte_carlo(Q_MC, Returns, count_state, count_state_action)\n",
    "\n",
    "V_MC = Q_MC.max(axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Monte Carlo -- plot\n",
    "s1 = np.arange(10)+1\n",
    "s2 = np.arange(21)+1\n",
    "ss1, ss2 = np.meshgrid(s1, s2, indexing='ij')\n",
    "\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "surf = ax.plot_surface(ss1, ss2, V_MC, cmap=cm.coolwarm)\n",
    "\n",
    "ax.set_xlabel(\"dealer's first card\")\n",
    "ax.set_ylabel(\"player's sum\")\n",
    "ax.set_zlabel(\"state value\")\n",
    "plt.yticks([1, 5, 10])\n",
    "plt.yticks([1, 7, 14, 21])\n",
    "fig.colorbar(surf, shrink=0.6)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARSA(lambda) function approximation -- coarse coding\n",
    "def coarse_coding(s, a):\n",
    "    v = np.zeros(3, dtype=int)\n",
    "    for i in range(3):\n",
    "        if (3 * i) <= s[0] <= (3 * (i+1)): v[i] = 1\n",
    "\n",
    "    v_ = np.zeros(6, dtype=int)\n",
    "    for i in range(6):\n",
    "        if (3 * i) <= s[1] <= (3 * i + 5): v_[i] = 1\n",
    "    v = np.append(v, v_)\n",
    "\n",
    "    v_ = np.zeros(2, dtype=int)\n",
    "    v_[a] = 1\n",
    "\n",
    "    return np.append(v, v_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARSA(lambda) function approximation -- one episode\n",
    "def SARSA_lambda_func_approx(w, decay):\n",
    "    epsilon = 0.05\n",
    "    stepsize = 0.01\n",
    "\n",
    "    s = env.reset()\n",
    "    elig_trace = np.zeros(len(w))\n",
    "\n",
    "    ## ================== change here ================== ##\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## ================================================= ##\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARSA(lambda) function approximation -- MSE vs. lambda\n",
    "n_episodes = 1000\n",
    "env = Easy21()\n",
    "\n",
    "Decay = np.arange(0, 1.1, 0.1)\n",
    "mse_Q_approx = np.zeros(len(Decay))\n",
    "\n",
    "n_state_action = 10 * 21 * 2\n",
    "\n",
    "feature = np.zeros([10, 21, 2, 11])\n",
    "for i in range(10):\n",
    "    for j in range(21):\n",
    "        for a in range(2):\n",
    "            feature[i, j, a, :] = coarse_coding([i, j], a)\n",
    "\n",
    "for i_dec in range(len(Decay)):\n",
    "    w = np.zeros(11)\n",
    "    for i_epi in range(n_episodes):\n",
    "        w = SARSA_lambda_func_approx(w, Decay[i_dec])\n",
    "\n",
    "    Q_SARSA_approx = np.zeros([10, 21, 2])\n",
    "    for i in range(10):\n",
    "        for j in range(21):\n",
    "            for a in range(2):\n",
    "                Q_SARSA_approx[i, j, a] = np.dot(w, feature[i, j, a, :])\n",
    "\n",
    "    mse_Q_approx[i_dec] = np.sum(np.square(Q_SARSA_approx - Q_MC)) / float(n_state_action)\n",
    "\n",
    "print(\"The best lambda is:\", Decay[mse_Q_approx.argmin()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARSA(lambda) function approximation -- MSE vs. lambda plot\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(Decay, mse_Q_approx, linewidth=2)\n",
    "\n",
    "ax.set_xlabel(\"decay factor\")\n",
    "ax.set_ylabel(\"mse of Q\")\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARSA(lambda) function approximation -- learning curve when lambda = 0 or 1\n",
    "mse_Q_approx_decay_0 = np.zeros(n_episodes)\n",
    "\n",
    "w = np.zeros(11)\n",
    "Q_SARSA_approx = np.zeros([10, 21, 2])\n",
    "for i_epi in range(n_episodes):\n",
    "    w = SARSA_lambda_func_approx(w, 0.)\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(21):\n",
    "            for a in range(2):\n",
    "                Q_SARSA_approx[i, j, a] = np.dot(w, feature[i, j, a, :])\n",
    "\n",
    "    mse_Q_approx_decay_0[i_epi] = np.sum(np.square(Q_SARSA_approx - Q_MC)) / float(n_state_action)\n",
    "\n",
    "\n",
    "mse_Q_approx_decay_1 = np.zeros(n_episodes)\n",
    "\n",
    "w = np.zeros(11)\n",
    "Q_SARSA_approx = np.zeros([10, 21, 2])\n",
    "for i_epi in range(n_episodes):\n",
    "    w = SARSA_lambda_func_approx(w, 1.)\n",
    "\n",
    "    for i in range(10):\n",
    "        for j in range(21):\n",
    "            for a in range(2):\n",
    "                Q_SARSA_approx[i, j, a] = np.dot(w, feature[i, j, a, :])\n",
    "\n",
    "    mse_Q_approx_decay_1[i_epi] = np.sum(np.square(Q_SARSA_approx - Q_MC)) / float(n_state_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SARSA(lambda) function approximation -- learning curve when lambda = 0 or 1 plot\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(np.arange(n_episodes), mse_Q_approx_decay_0, linewidth=2, c='olive', label='lambda = 0')\n",
    "ax.plot(np.arange(n_episodes), mse_Q_approx_decay_1, linewidth=2, c='salmon', label='lambda = 1')\n",
    "\n",
    "ax.set_xlabel(\"episode\")\n",
    "ax.set_ylabel(\"mse of Q\")\n",
    "ax.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
